##Multihead Attention Mechanism

If I have an input of size [1, 4, 512]. Here 1 represents batch size, 4 is max seq length/no. of max tokens in each batch and 512 is no of features/emb dim. Each token 
has 512 features.

1. Linear transformation: Changes input of size [1, 4, 512] to [1, 4, 1536].
   


